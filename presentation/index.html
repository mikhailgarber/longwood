<!DOCTYPE html>
<html>

<head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            overflow: hidden;
        }

        .presentation {
            height: 100vh;
            width: 100vw;
            position: relative;
            background: #fff;
        }

        .slide {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            padding: 2rem;
            opacity: 0;
            transform: translateX(100%);
            transition: all 0.4s ease-in-out;
        }

        .slide.active {
            opacity: 1;
            transform: translateX(0);
        }

        .slide.previous {
            transform: translateX(-100%);
        }

        .slide-content {
            max-width: 1200px;
            margin: 0 auto;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }

        /* Title Slide */
        .title-slide {
            text-align: center;
        }

        .title-slide h1 {
            font-size: 3.5rem;
            margin-bottom: 1rem;
            color: #2a4365;
        }

        .title-slide h2 {
            font-size: 2rem;
            color: #4a5568;
            font-weight: normal;
        }

        /* Content Slides */
        .slide h2 {
            font-size: 2.5rem;
            margin-bottom: 2rem;
            color: #2a4365;
        }

        .slide ul {
            list-style: none;
            margin-left: 1rem;
        }

        .slide li {
            margin-bottom: 1rem;
            font-size: 1.5rem;
            display: flex;
            align-items: center;
        }

        .slide li:before {
            content: "•";
            color: #4299e1;
            font-weight: bold;
            margin-right: 1rem;
        }

        /* Code Slides */
        .code-block {
            background: #1a202c;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 1.1rem;
            line-height: 1.5;
        }

        /* Demo Slides */
        .demo-placeholder {
            background: #f7fafc;
            border: 2px dashed #cbd5e0;
            border-radius: 8px;
            padding: 2rem;
            text-align: center;
            color: #718096;
            font-size: 1.2rem;
        }

        /* Mermaid Diagram */
        .mermaid {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100%;
            width: 100%;
        }

        /* Navigation */
        .nav {
            position: fixed;
            bottom: 2rem;
            left: 0;
            right: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 1rem;
            z-index: 100;
        }

        .nav button {
            background: #4299e1;
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background 0.3s ease;
        }

        .nav button:hover {
            background: #2b6cb0;
        }

        .nav button:disabled {
            background: #cbd5e0;
            cursor: not-allowed;
        }

        .slide-counter {
            font-size: 1rem;
            color: #4a5568;
        }

        /* Progress Bar */
        .progress {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
            background: #e2e8f0;
            z-index: 1000;
        }

        .progress-bar {
            height: 100%;
            background: #4299e1;
            width: 0;
            transition: width 0.3s ease;
        }
    </style>
</head>

<body>
    <div class="progress">
        <div class="progress-bar" id="progressBar"></div>
    </div>

    <div class="presentation">
        <!-- Title Slide -->
        <div class="slide active">
            <div class="slide-content title-slide">
                <h1>Computer Vision in Action</h1>
                <h2>From Motion Detection to Object Recognition</h2>
                <h3>Presentation by Mikhail Garber, for Longwood University</h3>
                <h4>February 18th, 2025</h4>
            </div>
        </div>

        <!-- Bio Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>About me</h2>
                <ul>
                    <li>Hi, my name is Mikhail Garber</li>
                    <li>Technology veteran with over 30 years of experience in software development and leadership</li>
                    <li>Led major projects at Microsoft, Salesforce, and Amazon, including AWS and satellite internet
                        initiatives</li>
                    <li>Successful entrepreneur - built and sold own software company</li>
                    <li>Currently serving as Principal Engineer at Rula Mental Health, developing online mental
                        healthcare platform</li>
                    <li>Actively engaged in pro bono work to give back to the technology community</li>
                    <li>You can find me at &nbsp; <a href="https://www.linkedin.com/in/mikhailgarber/">LinkedIn</a></li>
                    <li>This presentation and all the code is free to copy, modify and extend </li>
                    <li>Available at GitHub: &nbsp; <a
                            href="https://github.com/mikhailgarber/longwood">github.com/mikhailgarber/longwood</a></li>
                </ul>
            </div>
        </div>

        <!-- Computer Vision Introduction -->
        <div class="slide">
            <div class="slide-content">
                <h2>What is Computer Vision?</h2>
                <ul>
                    <li>Field of artificial intelligence that enables computers to understand and process visual
                        information from the world</li>
                    <li>Combines advanced algorithms, machine learning, and neural networks to interpret and analyze
                        digital images and videos</li>
                    <li>Applications span from facial recognition and autonomous vehicles to medical imaging and
                        industrial quality control</li>
                    <li>Modern computer vision achieves human-level accuracy in many tasks thanks to deep learning
                        breakthroughs</li>
                    <li>Market size in US expected to reach $60 billion by 2025, powering innovations across industries
                    </li>
                </ul>
            </div>
        </div>

        <!-- Presentation Overview -->
        <div class="slide">
            <div class="slide-content">
                <h2>What We'll Cover</h2>
                <ul>
                    <li>Fundamentals of Computer Vision Data Structures: Understanding the building blocks - pixels,
                        frames, and video streams - that form the foundation of all CV applications</li>
                    <li>Motion Detection Deep Dive: From theory to practice - how to detect and track movement in
                        real-time using frame differencing</li>
                    <li>Object Detection Exploration: Understanding how modern neural networks can identify and classify
                        objects in images and video streams using pre-trained models</li>
                    <li>Practical Implementations: Live demonstrations and code examples showing both applications in
                        action, with JavaScript implementations for computer vision</li>
                </ul>
            </div>
        </div>



        <!-- Pixel Data Structure -->
        <div class="slide">
            <div class="slide-content">
                <h2>Understanding Pixels</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>A pixel is the smallest unit of a digital image, represented as numeric values</li>
                            <li>In grayscale: Single value (0-255) representing brightness</li>
                            <li>In RGB: Three values representing Red, Green, and Blue intensities (each 0-255)</li>
                            <li>Additional channels like Alpha (transparency) may be present</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Pixel representations in JavaScript
const grayscale = 127;  // Medium gray

const rgb = {
  r: 255,  // Pure red
  g: 0,
  b: 0
};

const rgba = {
  r: 0,    // Semi-transparent green
  g: 255,
  b: 0,
  a: 0.5   // Alpha: 0 to 1
};</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Frame Structure -->
        <div class="slide">
            <div class="slide-content">
                <h2>Image Frames</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>A frame is a 2D array of pixels, forming a complete image</li>
                            <li>Dimensions described as width × height (e.g., 1920×1080)</li>
                            <li>Each pixel position accessed by x,y coordinates</li>
                            <li>Memory size = width × height × channels × bit depth</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Working with frames using Canvas
const canvas = document.createElement('canvas');
const ctx = canvas.getContext('2d');
canvas.width = 1920;
canvas.height = 1080;

// Get pixel data from canvas
const imageData = ctx.getImageData(
  0, 0, canvas.width, canvas.height
);
const pixels = imageData.data;  // Uint8ClampedArray

// Accessing a pixel (RGBA format)
const getPixel = (x, y) => {
  const i = (y * canvas.width + x) * 4;
  return {
    r: pixels[i],
    g: pixels[i + 1],
    b: pixels[i + 2],
    a: pixels[i + 3] / 255
  };
};</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Video Stream -->
        <div class="slide">
            <div class="slide-content">
                <h2>Video Streams</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>A video stream is a sequence of frames over time</li>
                            <li>Frame rate (FPS) determines frames captured per second</li>
                            <li>Real-time processing requires efficient frame handling</li>
                            <li>Streaming introduces concepts of buffering and latency</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Video stream using WebRTC
async function startVideoStream() {
  const stream = await navigator.mediaDevices
    .getUserMedia({ video: true });
  const video = document.createElement('video');
  video.srcObject = stream;

  // Process frames at 30 FPS
  setInterval(() => {
    // Draw video frame to canvas
    ctx.drawImage(video, 0, 0);
    
    // Get frame data for processing
    const frame = ctx.getImageData(
      0, 0, canvas.width, canvas.height
    );
    
    // Process frame here
    processFrame(frame);
  }, 1000 / 30);  // ~33ms per frame
}</pre>
                    </div>
                </div>
            </div>

        </div>


        <!-- Video Compression and Encoding -->
        <div class="slide">
            <div class="slide-content">
                <h2>Why Do We Need Compression?</h2>
                <ul style="margin-left: 2rem; margin-top: 0.25rem;">

                    <li>1080p frame = 1920 × 1080 pixels</li>
                    <li>Each pixel = 3 bytes (RGB) or 4 bytes (RGBA)</li>
                    <li>Single 1080p frame = 1920 × 1080 × 3 = 6.2 MB</li>
                    <li>At 30 FPS: 6.2 MB × 30 = 186 MB/second = 1.49 Gb/second</li>
                    <li>One minute of raw video = 11.16 GB!</li>

                    <li>H.264/AVC can achieve 95-98% size reduction while maintaining quality</li>
                    <li>1.49 Gb/s → 5-15 Mb/s after compression</li>
                    <li>One minute of compressed 1080p ≈ 75-225 MB (vs 11.16 GB raw)</li>

                    <li>Must decode compressed stream back to raw pixels for processing</li>
                    <li>Requires efficient pipeline: Decode → Transform → Process → Encode</li>
                    <li>Memory and CPU optimization crucial for real-time performance</li>

                </ul>
            </div>
        </div>



        <!-- Common Flow Diagram -->
        <div class="slide">
            <div class="slide-content">
                <h2>Common Processing Flow</h2>
                <div class="mermaid">
                    flowchart TD
                    VS[Video Stream<br />H.264/VP8/etc]
                    D1[Stream-to-Frames<br />Decoder]
                    T2[Computer Vision<br />Logic]
                    E1[Frames-to-Stream<br />Encoder]
                    Output[Output Stream]

                    VS --> D1
                    D1 --> |RGBA Pixels| T2
                    T2 -->|Processed Pixels| E1
                    E1 -->|Video Stream| Output

                    style VS fill:#f9f,stroke:#333
                    style T2 fill:#bfb,stroke:#333
                </div>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>FFmpeg: Universal Tool for Video Processing</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>FFmpeg is a powerful open-source multimedia framework for handling video, audio, and
                                other multimedia files</li>

                            <li>ffmpeg: Command-line tool for media conversion</li>
                            <li>ffplay: Media player for testing and debugging</li>
                            <li>ffprobe: Media stream analyzer</li>

                            <li>Video format conversion for preprocessing</li>
                            <li>Frame extraction for analysis</li>
                            <li>Real-time streaming and processing</li>

                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
    // Extract frames from video
    ffmpeg -i input.mp4 -vf fps=1 frame_%d.jpg
    
    // Convert video format for processing
    ffmpeg -i input.mp4 -c:v rawvideo \
           -pix_fmt rgb24 output.raw
    
    // Real-time streaming with processing
    ffmpeg -f v4l2 -i /dev/video0 \
           -vf "threshold" \
           -f sdl "Motion Detection"
    
    // Analyze video metadata
    ffprobe -v quiet -print_format json \
            -show_format -show_streams \
            input.mp4</pre>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Complete Processing Pipeline</h2>
                <ul>
                    <li>Stage 1 - Input Decoding:
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Input video stream (H.264, VP8, etc.) is fed into first FFmpeg process</li>
                            <li>FFmpeg decodes stream into sequence of raw RGBA frames</li>
                            <li>Uses rawvideo codec and rgba pixel format for maximum compatibility</li>
                        </ul>
                    </li>
                    <li>Stage 2 - CV Processing:
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Each raw frame is processed by custom CV application</li>
                            <li>Can implement motion detection, object recognition, or other CV logic</li>
                            <li>Operates on raw pixel data for maximum flexibility</li>
                        </ul>
                    </li>
                    <li>Stage 3 - Output Encoding:
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Processed frames are fed into second FFmpeg process</li>
                            <li>FFmpeg re-encodes frames into compressed video stream</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>


        <!-- Core Types -->
        <div class="slide">
            <div class="slide-content">
                <h2>Core Data Types</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>RGBA Frame: Complete frame of video with metadata</li>
                            <li>Detected Object: Output of object detection pipeline</li>
                            <li>Pixel: Simple x,y coordinate representation</li>
                            <li>These types form the foundation of our video processing system</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
interface RGBAFrame {
    sequence: number;      // Frame number
    width: number;        // Frame width
    height: number;       // Frame height
    data: Buffer;         // RGBA pixel data
    timestamp: number;    // Time in ms
}

interface DetectedObject {
    token: string;
    percent: number;
    x: number;
    y: number;
    width: number;
    height: number;
}

interface Pixel {
    x: number;           // X coordinate
    y: number;           // Y coordinate
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- FFmpeg Read Stream Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>Creating Video Read Stream</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Uses FFmpeg to decode input video stream into raw RGBA frames</li>
                            <li>Creates a transform stream that parses raw video data into frame objects</li>
                            <li>Configurable frame rate and resolution settings</li>
                            <li>Handles errors through FFmpeg stderr events</li>
                        </ul>
                    </div>
                    <div style="flex: 1.5;">
                        <pre class="code-block">
private createFFmpegReadStream(inputStream: Readable): Transform {
    const ffmpeg = spawn('ffmpeg', [
        '-i', 'pipe:0',           // Read from stdin
        '-f', 'rawvideo',         // Output raw video
        '-pix_fmt', 'rgba',       // RGBA pixel format
        '-s', `${this.options.width}x${this.options.height}`,
        '-r', String(this.options.fps),
        '-'                       // Write to stdout
    ]);

    // Create frame parser transform
    const frameParser = new FrameParser(
        this.options.width,
        this.options.height,
        this.options.fps
    );

    // Connect streams
    inputStream.pipe(ffmpeg.stdin);
    ffmpeg.stdout.pipe(frameParser);

    return frameParser;
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Transforming Video Stream to RGBA Frames</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Video streams must be parsed into individual RGBA frames for processing</li>
                            <li>Each frame contains width × height × 4 bytes (RGBA format)</li>
                            <li>Frames are tagged with sequence numbers and timestamps</li>
                            <li>Stream processing handles partial frame data using buffering</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
class FrameParser extends Transform {
  constructor(width, height, fps) {
    super({ readableObjectMode: true });
    this.frameSize = width * height * 4;
    this.buffer = Buffer.alloc(0);
    this.frameCount = 0;
  }
  _transform(chunk: Buffer, encoding: string, callback: Function) {
    // Append new data to existing buffer
    this.buffer = Buffer.concat([this.buffer, chunk]);
    // Process complete frames
    while (this.buffer.length >= this.frameSize) {
        const frameData = this.buffer.slice(0, this.frameSize);
        this.buffer = this.buffer.slice(this.frameSize);
        const sequence = this.frameCount++;
        const frame: RGBAFrame = {
            width: this.width,
            height: this.height,
            data: frameData,
            sequence,
            timestamp: (sequence * 1000) / this.fps // Calculate timestamp in ms
        };
        this.push(frame);
    }
    callback();
 } 
}
</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- FFmpeg Write Stream Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>Creating Video Write Stream</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Converts processed frames back into compressed video</li>
                            <li>Configurable output codec and bitrate settings</li>
                            <li>Uses yuv420p format for maximum compatibility</li>
                            <li>Enables streaming with proper MP4 fragmentation</li>
                        </ul>
                    </div>
                    <div style="flex: 1.5;">
                        <pre class="code-block">
private createFFmpegWriteStream(outputStream: Writable): Transform {
    const ffmpeg = spawn('ffmpeg', [
        '-f', 'rawvideo',
        '-vcodec', 'rawvideo',
        '-s', `${this.options.width}x${this.options.height}`,
        '-pix_fmt', 'rgba',
        '-framerate', String(this.options.fps),
        '-i', 'pipe:0',
        '-c:v', this.options.videoCodec,
        '-b:v', this.options.videoBitrate,
        '-pix_fmt', 'yuv420p',
        '-movflags', 'frag_keyframe+empty_moov',
        '-f', 'mp4',
        '-'
    ]);
    // Create frame serializer
    const frameSerializer = new Transform({
        objectMode: true,
        transform(frame: RGBAFrame, encoding, callback) {
            callback(null, frame.data);
        }
    });
    frameSerializer.pipe(ffmpeg.stdin);
    ffmpeg.stdout.pipe(outputStream);
    return frameSerializer;
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Custom Processing Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>Custom Frame Processing</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Flexible transform stream for custom frame processing</li>
                            <li>Works with RGBAFrame objects in object mode</li>
                            <li>Can modify pixel data for effects or analysis</li>
                            <li>Uses Node.js pipeline for proper error handling</li>
                        </ul>
                    </div>
                    <div style="flex: 1.5;">
                        <pre class="code-block">
public async process(
    inputStream: Readable,
    outputStream: Writable,
    frameProcessor?: Transform
): Promise<void> {
    try {
        const readStream = this.createFFmpegReadStream(inputStream);
        const writeStream = this.createFFmpegWriteStream(outputStream);
        const processor = frameProcessor || new Transform({
            objectMode: true,
            transform(frame: RGBAFrame, encoding, callback) {
                callback(null, frame);
            }
        });
        await pipeline(
            readStream,
            processor,
            writeStream
        );
    } catch (error) {
        console.error('Error during processing:', error);
        throw error;
    }
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- New Pipeline Setup Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>Setting Up the Processing Pipeline</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Create input/output streams for video processing</li>
                            <li>Configure VideoProcessor with desired parameters (FPS, dimensions)</li>
                            <li>Add transform streams for frame processing</li>
                            <li>Connect everything using Promise-based pipeline</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Create or setup I/O streams
const inputStream = createReadStream('input.mp4');
const outputStream = createWriteStream('output.mp4');

// Initialize processor with configuration
const processor = new VideoProcessor({
    fps: 30,
    width: 1280,
    height: 720
});

// Start processing pipeline
processor.process(
    inputStream,
    outputStream,
    new MyCustomTransformer()  // Transform stream
).then(() => {
    console.log('Processing complete');
}).catch(err => {
    console.error('Error:', err);
});</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Frame Processing Demo -->
        <div class="slide">
            <div class="slide-content">
                <h2>Demo: Color Inversion</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Demonstration of frame-by-frame processing</li>
                            <li>Each pixel's color channels are inverted: new_value = 255 - original_value</li>
                        </ul>
                        <pre class="code-block">
export class ColorInverter extends Transform {
    constructor() {
        super({
            objectMode: true,
            transform(frame, encoding, callback) {
                const newData = Buffer.from(frame.data);
                for (let i = 0; i < newData.length; i += 4) {
                    // Invert RGB values, leave alpha unchanged
                    newData[i] = 255 - newData[i];       // R
                    newData[i + 1] = 255 - newData[i + 1]; // G
                    newData[i + 2] = 255 - newData[i + 2]; // B
                }
                frame.data = newData;
                callback(null, frame);
            }
        });
    }
}</pre>
                    </div>
                    <div style="flex: 1; display: flex; flex-direction: column; gap: 1rem;">
                        <div>
                            <h3 style="margin-bottom: 0.25rem;">Original Video</h3>
                            <video src="street.mp4" controls style="width: 100%; border-radius: 4px;"></video>
                        </div>
                        <div>
                            <h3 style="margin-bottom: 0.25rem;">Inverted Colors</h3>
                            <video src="street-inverted.mp4" controls style="width: 100%; border-radius: 4px;"></video>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Motion Detection -->
        <div class="slide">
            <div class="slide-content">
                <h2>Our Motion Detection Implementation</h2>


                <ul style="margin-left: 1rem; margin-top: 0.25rem; margin-bottom: 0.25rem">

                    <li style="margin-bottom: 0.25rem">Stores the previous frame buffer for comparison with current
                        frame</li>

                    <li style="margin-bottom: 0.25rem">Each frame is processed as RGBA pixels (4 bytes per pixel:
                        Red, Green, Blue, Alpha)</li>
                    <li style="margin-bottom: 0.25rem">Calculates luminosity difference between corresponding pixels
                        in consecutive frames</li>
                    <li style="margin-bottom: 0.25rem">Uses threshold of 70 for luminosity difference to detect
                        significant motion</li>
                    <li style="margin-bottom: 0.25rem">Luminosity is calculated using industry-standard
                        coefficients: R(0.2126) + G(0.7152) + B(0.722)</li>
                    <li style="margin-bottom: 0.25rem">Performance optimization: Samples 25% of total pixels instead
                        of processing every pixel</li>
                    <li style="margin-bottom: 0.25rem">When motion is detected, highlights changed regions by
                        setting pixels to yellow (R:255, G:255, B:0)</li>
                    <li style="margin-bottom: 0.25rem">Maintains frame sequence and timestamp information for
                        accurate motion tracking</li>
                    <li style="margin-bottom: 0.25rem">Memory efficient: Processes frames in-place using Node.js
                        Buffer for pixel data</li>
                    <li style="margin-bottom: 0.25rem">Stream-based architecture: Integrates with Node.js Transform
                        streams for real-time processing</li>

                </ul>


            </div>
        </div>


        <!-- Motion Detection Code -->
        <div class="slide">
            <div class="slide-content">
                <h2>Motion Detection Implementation</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Compare current frame with previous frame</li>
                            <li>Mark changed pixels in yellow for visualization</li>
                            <li>Uses Buffer manipulation for direct pixel access</li>
                            <li>RGBA color values: Yellow = (255, 255, 0, 255)</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
public _transform(frame: any, encoding: string, 
    callback: TransformCallback): void {
    if (this.previousFrame) {
        const diff = this.differentPixels(
            this.previousFrame, frame.data, frame.width);
        const newData = Buffer.from(frame.data);
        for (let i = 0; i < diff.length; i += 4) {
            const pixel = diff[i];
            const index = (pixel.y * frame.width + pixel.x) * 4;
            newData[index] = 255;       // R
            newData[index + 1] = 255;   // G
            newData[index + 2] = 0;     // B
            newData[index + 3] = 255;   // A
        }
        this.previousFrame = frame.data;
        frame.data = newData;
    } else {
        this.previousFrame = frame.data;
    }
    callback(null, frame);
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Motion Detection Demo -->
        <div class="slide">
            <div class="slide-content">
                <h2>Demo: Motion Detection in Action</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <pre class="code-block">
private differentPixels(previousFrame: Buffer, 
currentFrame: Buffer, width: number): Pixel[] {
 const threshold = 70; // predefined level
 const totalPixels = previousFrame.length / 4;
 const step = Math.floor(totalPixels / 
 (totalPixels * 0.25)); // 25% of total pixels
 const pixels: Pixel[] = [];
 for (let i = 0; i < previousFrame.length; i += 4 * step) { 
    // assuming rgba format
    const prevLuminocity = this.luminocity(previousFrame[i], 
    previousFrame[i + 1], previousFrame[i + 2]);
    const currLuminocity = this.luminocity(currentFrame[i], 
    currentFrame[i + 1], currentFrame[i + 2]);
    if (Math.abs(prevLuminocity - currLuminocity) > threshold) {
        pixels.push({
            x: (i / 4) % width,
            y: Math.floor(i / 4 / width)
                    });
    }
 }
 return pixels;
}</pre>
                    </div>
                    <div style="flex: 1; display: flex; flex-direction: column; gap: 2rem;">
                        <div style="text-align: center;">
                            <h3 style="margin-bottom: 1rem;">Original Video</h3>
                            <video width="100%" controls>
                                <source src="street.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                        <div style="text-align: center;">
                            <h3 style="margin-bottom: 1rem;">Motion Detection</h3>
                            <video width="100%" controls>
                                <source src="street-motion-detection.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </div>



        <div class="slide">
            <div class="slide-content">
                <h2>Motion Detection: Practical Applications</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Real-time security monitoring with configurable sensitivity thresholds and automatic
                                alarm state management</li>
                            <li>Automated lighting control systems that respond to room occupancy for energy efficiency
                            </li>
                            <li>Industrial safety monitoring for detecting unauthorized movement in restricted zones
                            </li>
                            <li>Smart home automation triggers based on presence detection</li>
                            <li>Traffic monitoring systems for vehicle movement detection</li>
                            <li>Retail analytics for customer movement patterns</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
        function motionDetected(pixelCount: number): void {
            if (pixelCount > DETECTION_THRESHOLD) {
                // Significant motion detected
                if (state === "quiet") {
                    referenceCount += pixelCount;
                    if (referenceCount > ALARM_THRESHOLD) {
                        state = "alarm";
                        // Trigger alarm actions
                        referenceCount = 0;
                    }
                }
            } else {
                // Minimal or no motion
                if (state === "alarm") {
                    referenceCount += 1;
                    if (referenceCount > QUIET_THRESHOLD) {
                        state = "quiet";
                        referenceCount = 0;
                    }
                }
            }
        }</pre>
                    </div>
                </div>
            </div>
        </div>


        <!-- Object Detection Introduction -->
        <div class="slide">
            <div class="slide-content">
                <h2>Object Detection with OpenCV and YOLO</h2>
                <ul>
                    <li>OpenCV (Open Computer Vision):
                        <ul style="margin-left: 2rem; margin-top: 0.20rem;">
                            <li>Industry-standard framework for image processing</li>
                            <li>Provides essential tools for preprocessing and analysis</li>
                            <li>Optimized for real-time applications</li>
                            <li>Extensive library of algorithms for image transformation and feature detection</li>
                        </ul>
                    </li>
                    <li>YOLOv3 (You Only Look Once):
                        <ul style="margin-left: 2rem; margin-top: 0.20rem;">
                            <li>State-of-the-art real-time object detection system</li>
                            <li>Single neural network predicts bounding boxes and class probabilities</li>
                            <li>Can detect 80+ object classes in real-time (COCO dataset)</li>
                            <li>Processes images in a single forward pass, making it extremely fast</li>
                        </ul>
                    </li>
                    <li>Performance and Applications:
                        <ul style="margin-left: 2rem; margin-top: 0.20rem;">
                            <li>Achieves 45 FPS on modern GPUs while maintaining high accuracy</li>
                            <li>Effective at detecting small objects and objects in groups</li>
                            <li>Used in autonomous vehicles, surveillance systems, and retail analytics</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <!-- Neural Networks Introduction -->
        <div class="slide">
            <div class="slide-content">
                <h2>Understanding Neural Networks</h2>
                <ul>
                    <li>Neural networks are computer systems inspired by the human brain - they learn from examples just
                        like we do</li>
                    <li>They consist of digital neurons organized in layers, starting with an input layer that takes in
                        raw data (like pixel values from an image)</li>
                    <li>The middle "hidden" layers process and transform this data, detecting patterns and features
                        automatically</li>
                    <li>The output layer makes the final decision - for example, identifying an object as "this is a
                        cat"</li>
                    <li>Instead of being explicitly programmed with rules, neural networks learn by analyzing thousands
                        of examples</li>
                    <li>For instance, after seeing 10,000 pictures of cats and dogs, they learn to recognize distinctive
                        features like whiskers, ears, and tails</li>
                    <li>This learned knowledge then allows them to identify similar objects in new images they've never
                        seen before</li>
                    <li>This powerful ability to learn from examples makes neural networks ideal for tasks like object
                        recognition, face detection, and scene understanding</li>
                </ul>
            </div>
        </div>


        <!-- Object Detection Implementation -->
        <div class="slide">
            <div class="slide-content">
                <h2>Object Detection implementation - object_detector.cpp</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Implementation uses OpenCV's Deep Neural Network (DNN) module with YOLO (You Only Look
                                Once) model</li>
                            <li>Processes video frames as JPEG images through stdin, enabling stream processing</li>

                            <li>Real-time object detection with configurable confidence threshold</li>

                            <li>JSON output format for easy integration</li>

                            <li>Runs neural network inference</li>
                            <li>Processes detections and apply confidence filtering</li>
                            <li>Outputs bounding boxes with class labels and confidence scores</li>

                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Sample detection output format
[
    {
        "token": "person",
        "height": 380,
        "x": 250,
        "y": 120,
        "width": 200,
        "percent": 98
    },
    {
        "token": "car",
        "height": 150,
        "x": 400,
        "y": 300,
        "width": 250,
        "percent": 95
    }
]</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Object Detection Overview -->
        <div class="slide">
            <div class="slide-content">
                <h2>Object Detection Transform Overview</h2>
                <ul>
                    <li>Implements a Node.js Transform stream for real-time object detection</li>
                    <li>Uses a C++ binary (YOLO-based detector) for the actual detection</li>
                    <li>Processes RGBA frames and adds visual indicators for detected objects</li>
                    <li>Communicates via stdin/stdout with the C++ process</li>
                </ul>
                <pre class="code-block">
export class ObjectDetector extends Transform {
    constructor() {
        super({
            objectMode: true,
        });
        // Initialize object detector process
        this.objectDetectorProcess = spawn(objectDetectorPath, [
            modelNamesPath,
            modelConfigPath,
            modelWeightsPath,
            "0.6"  // confidence threshold
        ]);
    }
}</pre>
            </div>
        </div>

        <!-- Frame Processing Pipeline -->
        <div class="slide">
            <div class="slide-content">
                <h2>Frame Processing Pipeline</h2>
                <div class="split-content">
                    <div class="split-pane">
                        <ul>
                            <li>Each video frame goes through multiple stages:</li>
                            <li>1. Convert RGBA buffer to JPEG format</li>
                            <li>2. Send JPEG to C++ detector process</li>
                            <li>3. Receive and parse detection results</li>
                        </ul>
                    </div>
                    <div class="split-pane">
                        <pre class="code-block">
private async handleFrame(frame: RGBAFrame): Promise<RGBAFrame> {
    // Convert to JPEG for detector
    const jpegBuffer = await rgbaToJpeg(frame.data, {
        height: frame.height,
        width: frame.width,
        quality: 80
    });
    // Send to detector and await results
    const detections = await new Promise((resolve) => {
        this.odResolve = resolve;
        this.objectDetectorProcess.stdin.write(jpegBuffer);
    });
    // Process detections and modify frame
    let newData = Buffer.from(frame.data);
    // ... draw detection boxes ...
    return frame;
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Detection Results Processing -->
        <div class="slide">
            <div class="slide-content">
                <h2>Processing Detection Results</h2>
                <div class="split-content">
                    <div class="split-pane">
                        <ul>
                            <li>Detection results are used to draw yellow boxes</li>
                            <li>Text annotations added for each detection</li>
                        </ul>
                    </div>
                    <div class="split-pane">
                        <pre class="code-block">
for (const detection of detections) {
    const { x, y, width, height } = detection;
    // Draw horizontal lines
    for (let i = x; i < x + width; i++) {
        newData[(y * frame.width + i) * 4] = 255;     // R
        newData[(y * frame.width + i) * 4 + 1] = 255; // G
        newData[(y * frame.width + i) * 4 + 2] = 0;   // B
    }
    // ... draw vertical lines ...
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Error Handling and Cleanup -->
        <div class="slide">
            <div class="slide-content">
                <h2>Stream Management and Cleanup</h2>
                <div class="split-content">
                    <div class="split-pane">
                        <ul>
                            <li>Handles process stdout/stderr streams</li>
                            <li>Accumulates output chunks until complete JSON</li>
                            <li>Proper process cleanup on stop</li>
                        </ul>
                    </div>
                    <div class="split-pane">
                        <pre class="code-block">
this.objectDetectorProcess.stdout.on('data', (chunk) => {
    const outputString = chunk.toString();
    this.chunks.push(chunk);
    if (outputString.indexOf(']') > -1) {
        const obj = JSON.parse(
            Buffer.concat(this.chunks).toString()
        );
        this.chunks = [];
        if (this.odResolve !== (() => {})) {
            this.odResolve(obj);
            this.odResolve = () => {};
        }
    }
});
public stop(): void {
    console.log('Stopping Object Detector...');
    this.objectDetectorProcess.kill();
}</pre>
                    </div>
                </div>
            </div>

        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Object Detection Demo</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Real-time object detection using YOLOv3 neural network model</li>
                            <li>Detects and classifies 80+ object types including people, vehicles, and common objects
                            </li>
                            <li>Achieves 30+ FPS processing speed on modern hardware</li>
                            <li>Highlights detected objects with yellow bounding boxes</li>
                            <li>Labels each detection with object class and confidence score</li>
                        </ul>
                    </div>
                    <div style="flex: 2;">
                        <div style="display: flex; flex-direction: column; gap: 1rem;">
                            <div>
                                <h3 class="text-lg font-semibold mb-2">Original Video</h3>
                                <video src="street.mp4" controls
                                    style="width: 70%; height: auto; border-radius: 8px;"></video>
                            </div>
                            <div>
                                <h3 class="text-lg font-semibold mb-2">With Object Detection</h3>
                                <video src="street-object-detection.mp4" controls
                                    style="width: 70%; height: auto; border-radius: 8px;"></video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Intelligent Scene Understanding</h2>
                <ul>
                    <li>Scene Understanding: Maintains an active scene model by tracking detected objects and their
                        changes over time, enabling continuous monitoring of the environment</li>
                    <li>Confidence Filtering: Implements an 80% confidence threshold to ensure reliable detections and
                        minimize false positives, crucial for real-world applications</li>
                    <li>Object Persistence: Uses sophisticated size comparison (10% threshold) to track objects across
                        frames, determining whether objects are new or existing in the scene</li>
                    <li>Change Detection: Actively monitors object entry and exit from the scene, providing real-time
                        awareness of environmental changes</li>
                    <li>Real-world Applications: Powers security systems, retail analytics, automated inventory
                        management, and smart surveillance solutions</li>
                </ul>
            </div>
        </div>

        <!-- Object Detection Implementation -->
        <div class="slide">
            <div class="slide-content">
                <h2>Scene Management Implementation</h2>
                <pre class="code-block">
export function objectsDetected(objects: DetectedObject[]): void {
    const previousScene = [...scene];
    for (const object of objects) {
        if (object.percent > CONFIDENT_THRESHOLD) {
            const existingObjects = scene.filter(o => o.token === object.token);
            let found = false;
            if (existingObjects.length > 0) {
                for (const existingObject of existingObjects) {
                    const sizeDiff = Math.abs(existingObject.width - object.width) / existingObject.width;
                    if (sizeDiff < SIZE_THRESHOLD) {
                        previousScene.splice(previousScene.indexOf(existingObject), 1);
                        scene[scene.indexOf(existingObject)] = object;
                        found = true;
                    }
                }
            }
            if (!found) {
                console.log(`New object detected: ${object.token}`);
                scene.push(object);
            }
        }
    }
    // remove objects still in the previous scene from scene
    ...
}</pre>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Cloud-Based Computer Vision Solutions</h2>
                <ul>
                    <li>Amazon Rekognition: Pre-trained deep learning models for image and video analysis, including
                        facial recognition, object detection, and text extraction</li>
                    <li>Google Cloud Vision AI: Advanced ML models for image classification, object detection, OCR, and
                        explicit content detection</li>
                    <li>Microsoft Azure Computer Vision: Comprehensive suite of vision APIs for image analysis, face
                        detection, and custom model training</li>
                    <li>Smart Security Cameras: Arlo, Nest, and Ring offer built-in CV capabilities for motion detection
                        and object recognition</li>
                    <li>Edge Computing Solutions: AWS Panorama and Google Coral enable on-premise processing with cloud
                        integration</li>
                    <li>Specialized APIs: Services like Clarifai and CloudSight offer domain-specific computer vision
                        models</li>
                </ul>
            </div>
        </div>

        <!-- Limitations of Cloud-Based CV -->
        <div class="slide">
            <div class="slide-content">
                <h2>Limitations of Cloud-Based Computer Vision</h2>
                <ul>
                    <li>Cost becomes prohibitive at scale due to pay-per-use pricing, bandwidth costs, and
                        enterprise-level contract requirements</li>
                    <li>Privacy concerns arise from sending sensitive data outside your network, along with GDPR/HIPAA
                        compliance challenges and limited data control</li>
                    <li>Vendor lock-in through proprietary APIs and closed-source solutions makes migration difficult
                        and limits customization options</li>
                    <li>Performance suffers from network latency and connectivity dependencies, impacting real-time
                        processing capabilities</li>
                </ul>
            </div>
        </div>

        <!-- Face Detection and Privacy -->
        <div class="slide">
            <div class="slide-content">
                <h2>Face Detection (NOT in this presentation...)</h2>
                <ul>
                    <li>Face detection uses cascade classifiers or deep learning to identify and track facial features
                        in images and video</li>
                    <li>Technology can detect multiple faces simultaneously and provide real-time tracking of location,
                        size, and orientation</li>
                    <li>Biometric data from face detection requires special handling under privacy regulations like GDPR
                        and CCPA</li>
                    <li>Organizations must obtain clear consent and establish data retention policies for facial data
                    </li>
                    <li>Best practices include data minimization, opt-out mechanisms, and secure storage protocols</li>
                    <li>Regular security audits help ensure ongoing protection of sensitive facial recognition data</li>
                    <li>Consider ethical implications when implementing face detection in public spaces</li>
                </ul>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Questions?</h2>
                <ul>
                    <li>...</li>
                </ul>
                <h2>Parting Thoughts?</h2>
                <ul>
                    <li>...</li>
                </ul>

            </div>
        </div>

        <div class="nav">
            <button onclick="previousSlide()" id="prevBtn">Previous</button>
            <span class="slide-counter" id="slideCounter">1 / 10</span>
            <button onclick="nextSlide()" id="nextBtn">Next</button>
        </div>

        <script>
            // Initialize Mermaid
            mermaid.initialize({ startOnLoad: true });

            let currentSlide = 0;
            const slides = document.querySelectorAll('.slide');
            const totalSlides = slides.length;
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');
            const counter = document.getElementById('slideCounter');
            const progressBar = document.getElementById('progressBar');

            function updateSlides() {
                slides.forEach((slide, index) => {
                    slide.classList.remove('active', 'previous');
                    if (index === currentSlide) {
                        slide.classList.add('active');
                    } else if (index < currentSlide) {
                        slide.classList.add('previous');
                    }
                });

                // Update counter and buttons
                counter.textContent = `${currentSlide + 1} / ${totalSlides}`;
                prevBtn.disabled = currentSlide === 0;
                nextBtn.disabled = currentSlide === totalSlides - 1;

                // Update progress bar
                const progress = ((currentSlide + 1) / totalSlides) * 100;
                progressBar.style.width = `${progress}%`;
            }

            function nextSlide() {
                if (currentSlide < totalSlides - 1) {
                    currentSlide++;
                    updateSlides();
                }
            }

            function previousSlide() {
                if (currentSlide > 0) {
                    currentSlide--;
                    updateSlides();
                }
            }

            // Keyboard navigation
            document.addEventListener('keydown', (e) => {
                if (e.key === 'ArrowRight' || e.key === ' ') {
                    nextSlide();
                } else if (e.key === 'ArrowLeft') {
                    previousSlide();
                }
            });

            // Initialize progress bar
            updateSlides();
        </script>
</body>

</html>
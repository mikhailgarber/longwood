<!DOCTYPE html>
<html>

<head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            overflow: hidden;
        }

        .presentation {
            height: 100vh;
            width: 100vw;
            position: relative;
            background: #fff;
        }

        .slide {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            padding: 2rem;
            opacity: 0;
            transform: translateX(100%);
            transition: all 0.4s ease-in-out;
        }

        .slide.active {
            opacity: 1;
            transform: translateX(0);
        }

        .slide.previous {
            transform: translateX(-100%);
        }

        .slide-content {
            max-width: 1200px;
            margin: 0 auto;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }

        /* Title Slide */
        .title-slide {
            text-align: center;
        }

        .title-slide h1 {
            font-size: 3.5rem;
            margin-bottom: 1rem;
            color: #2a4365;
        }

        .title-slide h2 {
            font-size: 2rem;
            color: #4a5568;
            font-weight: normal;
        }

        /* Content Slides */
        .slide h2 {
            font-size: 2.5rem;
            margin-bottom: 2rem;
            color: #2a4365;
        }

        .slide ul {
            list-style: none;
            margin-left: 1rem;
        }

        .slide li {
            margin-bottom: 1rem;
            font-size: 1.5rem;
            display: flex;
            align-items: center;
        }

        .slide li:before {
            content: "•";
            color: #4299e1;
            font-weight: bold;
            margin-right: 1rem;
        }

        /* Code Slides */
        .code-block {
            background: #1a202c;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 1.1rem;
            line-height: 1.5;
        }

        /* Demo Slides */
        .demo-placeholder {
            background: #f7fafc;
            border: 2px dashed #cbd5e0;
            border-radius: 8px;
            padding: 2rem;
            text-align: center;
            color: #718096;
            font-size: 1.2rem;
        }

        /* Mermaid Diagram */
        .mermaid {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100%;
            width: 100%;
        }

        /* Navigation */
        .nav {
            position: fixed;
            bottom: 2rem;
            left: 0;
            right: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 1rem;
            z-index: 100;
        }

        .nav button {
            background: #4299e1;
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background 0.3s ease;
        }

        .nav button:hover {
            background: #2b6cb0;
        }

        .nav button:disabled {
            background: #cbd5e0;
            cursor: not-allowed;
        }

        .slide-counter {
            font-size: 1rem;
            color: #4a5568;
        }

        /* Progress Bar */
        .progress {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
            background: #e2e8f0;
            z-index: 1000;
        }

        .progress-bar {
            height: 100%;
            background: #4299e1;
            width: 0;
            transition: width 0.3s ease;
        }
    </style>
</head>

<body>
    <div class="progress">
        <div class="progress-bar" id="progressBar"></div>
    </div>

    <div class="presentation">
        <!-- Title Slide -->
        <div class="slide active">
            <div class="slide-content title-slide">
                <h1>Computer Vision in Action</h1>
                <h2>From Motion Detection to Object Recognition</h2>
                <h3>Presentation by Mikhail Garber, for Longwood University</h3>
                <h4>February 18th, 2025</h4>
            </div>
        </div>

        <!-- Bio Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>About me</h2>
                <ul>
                    <li>Hi, my name is Mikhail Garber</li>
                    <li>Technology veteran with over 30 years of experience in software development and leadership</li>
                    <li>Led major projects at Microsoft, Salesforce, and Amazon, including AWS and satellite internet
                        initiatives</li>
                    <li>Successful entrepreneur - built and sold own software company</li>
                    <li>Currently serving as Principal Engineer at Rula Mental Health, developing online mental
                        healthcare platform</li>
                    <li>Actively engaged in pro bono work to give back to the technology community</li>
                    <li>You can find me at &nbsp; <a href="https://www.linkedin.com/in/mikhailgarber/">LinkedIn</a></li>
                    <li>This presentation and all the code is free to copy, modify and extend </li>
                    <li>Available at GitHub: &nbsp; <a
                            href="https://github.com/mikhailgarber/longwood">github.com/mikhailgarber/longwood</a></li>
                </ul>
            </div>
        </div>

        <!-- Computer Vision Introduction -->
        <div class="slide">
            <div class="slide-content">
                <h2>What is Computer Vision?</h2>
                <ul>
                    <li>Field of artificial intelligence that enables computers to understand and process visual
                        information from the world</li>
                    <li>Combines advanced algorithms, machine learning, and neural networks to interpret and analyze
                        digital images and videos</li>
                    <li>Applications span from facial recognition and autonomous vehicles to medical imaging and
                        industrial quality control</li>
                    <li>Modern computer vision achieves human-level accuracy in many tasks thanks to deep learning
                        breakthroughs</li>
                    <li>Market size in US expected to reach $60 billion by 2025, powering innovations across industries
                    </li>
                </ul>
            </div>
        </div>

        <!-- Presentation Overview -->
        <div class="slide">
            <div class="slide-content">
                <h2>What We'll Cover</h2>
                <ul>
                    <li>Fundamentals of Computer Vision Data Structures: Understanding the building blocks - pixels,
                        frames, and video streams - that form the foundation of all CV applications</li>
                    <li>Motion Detection Deep Dive: From theory to practice - how to detect and track movement in
                        real-time using frame differencing</li>
                    <li>Object Detection Exploration: Understanding how modern neural networks can identify and classify
                        objects in images and video streams using pre-trained models</li>
                    <li>Practical Implementations: Live demonstrations and code examples showing both applications in
                        action, with JavaScript implementations for computer vision</li>
                </ul>
            </div>
        </div>



        <!-- Pixel Data Structure -->
        <div class="slide">
            <div class="slide-content">
                <h2>Understanding Pixels</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>A pixel is the smallest unit of a digital image, represented as numeric values</li>
                            <li>In grayscale: Single value (0-255) representing brightness</li>
                            <li>In RGB: Three values representing Red, Green, and Blue intensities (each 0-255)</li>
                            <li>Additional channels like Alpha (transparency) may be present</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Pixel representations in JavaScript
const grayscale = 127;  // Medium gray

const rgb = {
  r: 255,  // Pure red
  g: 0,
  b: 0
};

const rgba = {
  r: 0,    // Semi-transparent green
  g: 255,
  b: 0,
  a: 0.5   // Alpha: 0 to 1
};</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Frame Structure -->
        <div class="slide">
            <div class="slide-content">
                <h2>Image Frames</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>A frame is a 2D array of pixels, forming a complete image</li>
                            <li>Dimensions described as width × height (e.g., 1920×1080)</li>
                            <li>Each pixel position accessed by x,y coordinates</li>
                            <li>Memory size = width × height × channels × bit depth</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Working with frames using Canvas
const canvas = document.createElement('canvas');
const ctx = canvas.getContext('2d');
canvas.width = 1920;
canvas.height = 1080;

// Get pixel data from canvas
const imageData = ctx.getImageData(
  0, 0, canvas.width, canvas.height
);
const pixels = imageData.data;  // Uint8ClampedArray

// Accessing a pixel (RGBA format)
const getPixel = (x, y) => {
  const i = (y * canvas.width + x) * 4;
  return {
    r: pixels[i],
    g: pixels[i + 1],
    b: pixels[i + 2],
    a: pixels[i + 3] / 255
  };
};</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Video Stream -->
        <div class="slide">
            <div class="slide-content">
                <h2>Video Streams</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>A video stream is a sequence of frames over time</li>
                            <li>Frame rate (FPS) determines frames captured per second</li>
                            <li>Real-time processing requires efficient frame handling</li>
                            <li>Streaming introduces concepts of buffering and latency</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Video stream using WebRTC
async function startVideoStream() {
  const stream = await navigator.mediaDevices
    .getUserMedia({ video: true });
  const video = document.createElement('video');
  video.srcObject = stream;

  // Process frames at 30 FPS
  setInterval(() => {
    // Draw video frame to canvas
    ctx.drawImage(video, 0, 0);
    
    // Get frame data for processing
    const frame = ctx.getImageData(
      0, 0, canvas.width, canvas.height
    );
    
    // Process frame here
    processFrame(frame);
  }, 1000 / 30);  // ~33ms per frame
}</pre>
                    </div>
                </div>
            </div>

        </div>


        <!-- Video Compression and Encoding -->
        <div class="slide">
            <div class="slide-content">
                <h2>Why Do We Need Compression?</h2>
                <ul style="margin-left: 2rem; margin-top: 0.25rem;">

                    <li>1080p frame = 1920 × 1080 pixels</li>
                    <li>Each pixel = 3 bytes (RGB) or 4 bytes (RGBA)</li>
                    <li>Single 1080p frame = 1920 × 1080 × 3 = 6.2 MB</li>
                    <li>At 30 FPS: 6.2 MB × 30 = 186 MB/second = 1.49 Gb/second</li>
                    <li>One minute of raw video = 11.16 GB!</li>

                    <li>H.264/AVC can achieve 95-98% size reduction while maintaining quality</li>
                    <li>1.49 Gb/s → 5-15 Mb/s after compression</li>
                    <li>One minute of compressed 1080p ≈ 75-225 MB (vs 11.16 GB raw)</li>

                    <li>Must decode compressed stream back to raw pixels for processing</li>
                    <li>Requires efficient pipeline: Decode → Transform → Process → Encode</li>
                    <li>Memory and CPU optimization crucial for real-time performance</li>

                </ul>
            </div>
        </div>



        <!-- Common Flow Diagram -->
        <div class="slide">
            <div class="slide-content">
                <h2>Common Processing Flow</h2>
                <div class="mermaid">
                    flowchart LR
                    subgraph Input
                    VS[Video Stream<br />H.264/VP8/etc]
                    end

                    subgraph Decoder
                    D1[Stream-to-Frames Decoder]
                    D2[Raw Frame Buffer]
                    D1 --> D2
                    end

                    subgraph Transformer
                    T1[Frame Processor]
                    T2[Computer Vision<br />Logic]
                    T1 --> T2
                    end

                    subgraph Encoder
                    E1[Frames-to-Stream Encoder]
                    end

                    VS --> D1
                    D2 --> |RGBA Pixels| T1
                    T2 --> |Processed Pixels| E1
                    E1 --> |Video Stream| Output

                    style VS fill:#f9f,stroke:#333
                    style D2 fill:#bbf,stroke:#333
                    style T2 fill:#bfb,stroke:#333
                </div>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>FFmpeg: Universal Tool for Video Processing</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>FFmpeg is a powerful open-source multimedia framework for handling video, audio, and
                                other multimedia files</li>
                            <li>Key Components:
                                <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                                    <li>ffmpeg: Command-line tool for media conversion</li>
                                    <li>ffplay: Media player for testing and debugging</li>
                                    <li>ffprobe: Media stream analyzer</li>
                                </ul>
                            </li>
                            <li>Common CV Applications:
                                <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                                    <li>Video format conversion for preprocessing</li>
                                    <li>Frame extraction for analysis</li>
                                    <li>Real-time streaming and processing</li>
                                </ul>
                            </li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
    // Extract frames from video
    ffmpeg -i input.mp4 -vf fps=1 frame_%d.jpg
    
    // Convert video format for processing
    ffmpeg -i input.mp4 -c:v rawvideo \
           -pix_fmt rgb24 output.raw
    
    // Real-time streaming with processing
    ffmpeg -f v4l2 -i /dev/video0 \
           -vf "threshold" \
           -f sdl "Motion Detection"
    
    // Analyze video metadata
    ffprobe -v quiet -print_format json \
            -show_format -show_streams \
            input.mp4</pre>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Complete Processing Pipeline</h2>
                <ul>
                    <li>Stage 1 - Input Decoding:
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Input video stream (H.264, VP8, etc.) is fed into first FFmpeg process</li>
                            <li>FFmpeg decodes stream into sequence of raw RGBA frames</li>
                            <li>Uses rawvideo codec and rgba pixel format for maximum compatibility</li>
                        </ul>
                    </li>
                    <li>Stage 2 - CV Processing:
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Each raw frame is processed by custom CV application</li>
                            <li>Can implement motion detection, object recognition, or other CV logic</li>
                            <li>Operates on raw pixel data for maximum flexibility</li>
                        </ul>
                    </li>
                    <li>Stage 3 - Output Encoding:
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Processed frames are fed into second FFmpeg process</li>
                            <li>FFmpeg re-encodes frames into compressed video stream</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>


        <!-- Core Types -->
        <div class="slide">
            <div class="slide-content">
                <h2>Core Data Types</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>RGBA Frame: Complete frame of video with metadata</li>
                            <li>Detected Object: Output of object detection pipeline</li>
                            <li>Pixel: Simple x,y coordinate representation</li>
                            <li>These types form the foundation of our video processing system</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
interface RGBAFrame {
    sequence: number;      // Frame number
    width: number;        // Frame width
    height: number;       // Frame height
    data: Buffer;         // RGBA pixel data
    timestamp: number;    // Time in ms
}

interface DetectedObject {
    token: string;
    percent: number;
    x: number;
    y: number;
    width: number;
    height: number;
}

interface Pixel {
    x: number;           // X coordinate
    y: number;           // Y coordinate
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- FFmpeg Read Stream Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>Creating Video Read Stream</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Uses FFmpeg to decode input video stream into raw RGBA frames</li>
                            <li>Creates a transform stream that parses raw video data into frame objects</li>
                            <li>Configurable frame rate and resolution settings</li>
                            <li>Handles errors through FFmpeg stderr events</li>
                        </ul>
                    </div>
                    <div style="flex: 1.5;">
                        <pre class="code-block">
private createFFmpegReadStream(inputStream: Readable): Transform {
    const ffmpeg = spawn('ffmpeg', [
        '-i', 'pipe:0',           // Read from stdin
        '-f', 'rawvideo',         // Output raw video
        '-pix_fmt', 'rgba',       // RGBA pixel format
        '-s', `${this.options.width}x${this.options.height}`,
        '-r', String(this.options.fps),
        '-'                       // Write to stdout
    ]);

    // Create frame parser transform
    const frameParser = new FrameParser(
        this.options.width,
        this.options.height,
        this.options.fps
    );

    // Connect streams
    inputStream.pipe(ffmpeg.stdin);
    ffmpeg.stdout.pipe(frameParser);

    return frameParser;
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <div class="slide">
            <div class="slide-content">
                <h2>Transforming Video Stream to RGBA Frames</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Video streams must be parsed into individual RGBA frames for processing</li>
                            <li>Each frame contains width × height × 4 bytes (RGBA format)</li>
                            <li>Frames are tagged with sequence numbers and timestamps</li>
                            <li>Stream processing handles partial frame data using buffering</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
class FrameParser extends Transform {
  constructor(width, height, fps) {
    super({ readableObjectMode: true });
    this.frameSize = width * height * 4;
    this.buffer = Buffer.alloc(0);
    this.frameCount = 0;
  }
  _transform(chunk: Buffer, encoding: string, callback: Function) {
    // Append new data to existing buffer
    this.buffer = Buffer.concat([this.buffer, chunk]);
    // Process complete frames
    while (this.buffer.length >= this.frameSize) {
        const frameData = this.buffer.slice(0, this.frameSize);
        this.buffer = this.buffer.slice(this.frameSize);
        const sequence = this.frameCount++;
        const frame: RGBAFrame = {
            width: this.width,
            height: this.height,
            data: frameData,
            sequence,
            timestamp: (sequence * 1000) / this.fps // Calculate timestamp in ms
        };
        this.push(frame);
    }
    callback();
 } 
}
</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- FFmpeg Write Stream Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>Creating Video Write Stream</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Converts processed frames back into compressed video</li>
                            <li>Configurable output codec and bitrate settings</li>
                            <li>Uses yuv420p format for maximum compatibility</li>
                            <li>Enables streaming with proper MP4 fragmentation</li>
                        </ul>
                    </div>
                    <div style="flex: 1.5;">
                        <pre class="code-block">
private createFFmpegWriteStream(outputStream: Writable): Transform {
    const ffmpeg = spawn('ffmpeg', [
        '-f', 'rawvideo',
        '-vcodec', 'rawvideo',
        '-s', `${this.options.width}x${this.options.height}`,
        '-pix_fmt', 'rgba',
        '-framerate', String(this.options.fps),
        '-i', 'pipe:0',
        '-c:v', this.options.videoCodec,
        '-b:v', this.options.videoBitrate,
        '-pix_fmt', 'yuv420p',
        '-movflags', 'frag_keyframe+empty_moov',
        '-f', 'mp4',
        '-'
    ]);

    // Create frame serializer
    const frameSerializer = new Transform({
        objectMode: true,
        transform(frame: RGBAFrame, encoding, callback) {
            callback(null, frame.data);
        }
    });

    frameSerializer.pipe(ffmpeg.stdin);
    ffmpeg.stdout.pipe(outputStream);

    return frameSerializer;
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Custom Processing Slide -->
        <div class="slide">
            <div class="slide-content">
                <h2>Custom Frame Processing</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Flexible transform stream for custom frame processing</li>
                            <li>Works with RGBAFrame objects in object mode</li>
                            <li>Can modify pixel data for effects or analysis</li>
                            <li>Uses Node.js pipeline for proper error handling</li>
                        </ul>
                    </div>
                    <div style="flex: 1.5;">
                        <pre class="code-block">
public async process(
    inputStream: Readable,
    outputStream: Writable,
    frameProcessor?: Transform
): Promise<void> {
    try {
        // Create processing pipeline
        const readStream = this.createFFmpegReadStream(inputStream);
        const writeStream = this.createFFmpegWriteStream(outputStream);

        // Default pass-through processor if none provided
        const processor = frameProcessor || new Transform({
            objectMode: true,
            transform(frame: RGBAFrame, encoding, callback) {
                callback(null, frame);
            }
        });

        // Connect all streams
        await pipeline(
            readStream,
            processor,
            writeStream
        );
    } catch (error) {
        console.error('Error during processing:', error);
        throw error;
    }
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Frame Processing Demo -->
        <div class="slide">
            <div class="slide-content">
                <h2>Demo: Color Inversion</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Simple yet effective demonstration of frame-by-frame processing</li>
                            <li>Each pixel's color channels are inverted: new_value = 255 - original_value</li>
                            <li>Maintains original video's resolution and frame rate</li>
                            <li>Real-time processing possible with efficient implementation</li>
                        </ul>
                        <pre class="code-block">
// Color inversion processing
export class ColorInverter extends Transform {
    constructor() {
        super({
            objectMode: true,
            transform(frame, encoding, callback) {
                const newData = Buffer.from(frame.data);
                for (let i = 0; i < newData.length; i += 4) {
                    // Invert RGB values, leave alpha unchanged
                    newData[i] = 255 - newData[i];       // R
                    newData[i + 1] = 255 - newData[i + 1]; // G
                    newData[i + 2] = 255 - newData[i + 2]; // B
                }
                frame.data = newData;
                callback(null, frame);
            }
        });
    }
}</pre>
                    </div>
                    <div style="flex: 1; display: flex; flex-direction: column; gap: 1rem;">
                        <div>
                            <h3 style="margin-bottom: 0.25rem;">Original Video</h3>
                            <video src="street.mp4" controls style="width: 100%; border-radius: 4px;"></video>
                        </div>
                        <div>
                            <h3 style="margin-bottom: 0.25rem;">Inverted Colors</h3>
                            <video src="street-inverted.mp4" controls style="width: 100%; border-radius: 4px;"></video>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Motion Detection -->
        <div class="slide">
            <div class="slide-content">
                <h2>Our Motion Detection Implementation</h2>


                <ul style="margin-left: 1rem; margin-top: 0.25rem; margin-bottom: 0.25rem">

                    <li style="margin-bottom: 0.25rem">Stores the previous frame buffer for comparison with current
                        frame</li>

                    <li style="margin-bottom: 0.25rem">Each frame is processed as RGBA pixels (4 bytes per pixel:
                        Red, Green, Blue, Alpha)</li>
                    <li style="margin-bottom: 0.25rem">Calculates luminosity difference between corresponding pixels
                        in consecutive frames</li>
                    <li style="margin-bottom: 0.25rem">Uses threshold of 70 for luminosity difference to detect
                        significant motion</li>
                    <li style="margin-bottom: 0.25rem">Luminosity is calculated using industry-standard
                        coefficients: R(0.2126) + G(0.7152) + B(0.722)</li>
                    <li style="margin-bottom: 0.25rem">Performance optimization: Samples 25% of total pixels instead
                        of processing every pixel</li>
                    <li style="margin-bottom: 0.25rem">When motion is detected, highlights changed regions by
                        setting pixels to yellow (R:255, G:255, B:0)</li>
                    <li style="margin-bottom: 0.25rem">Maintains frame sequence and timestamp information for
                        accurate motion tracking</li>
                    <li style="margin-bottom: 0.25rem">Memory efficient: Processes frames in-place using Node.js
                        Buffer for pixel data</li>
                    <li style="margin-bottom: 0.25rem">Stream-based architecture: Integrates with Node.js Transform
                        streams for real-time processing</li>

                </ul>


            </div>
        </div>


        <!-- Motion Detection Code -->
        <div class="slide">
            <div class="slide-content">
                <h2>Motion Detection Implementation</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Compare current frame with previous frame</li>
                            <li>Mark changed pixels in yellow for visualization</li>
                            <li>Uses Buffer manipulation for direct pixel access</li>
                            <li>RGBA color values: Yellow = (255, 255, 0, 255)</li>
                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
private handleFrame(frame: RGBAFrame): RGBAFrame {
    if (this.previousFrame) {
        // Get pixels that changed vs previous frame
        const diff = this.differentPixels(
            this.previousFrame, 
            frame.data, 
            frame.width
        );
        
        // Create new buffer for modified frame
        const newData = Buffer.from(frame.data);
        
        // Paint changed pixels yellow
        for (let i = 0; i < diff.length; i += 4) {
            const pixel = diff[i];
            const index = (pixel.y * frame.width + pixel.x) * 4;
            newData[index] = 255;     // R
            newData[index + 1] = 255; // G
            newData[index + 2] = 0;   // B
            newData[index + 3] = 255; // A
        }
        
        this.previousFrame = frame.data;
        frame.data = newData;
    } else {
        this.previousFrame = frame.data;
    }
    return frame;
}</pre>
                    </div>
                </div>
            </div>
        </div>

        <!-- Motion Detection Demo -->
        <div class="slide">
            <div class="slide-content">
                <h2>Demo: Motion Detection in Action</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <pre class="code-block">
private differentPixels(previousFrame: Buffer, 
currentFrame: Buffer, width: number): Pixel[] {
 const threshold = 70; // predefined level
 const totalPixels = previousFrame.length / 4;
 const step = Math.floor(totalPixels / 
 (totalPixels * 0.25)); // 25% of total pixels
 const pixels: Pixel[] = [];
 for (let i = 0; i < previousFrame.length; i += 4 * step) { 
    // assuming rgba format
    const prevLuminocity = this.luminocity(previousFrame[i], 
    previousFrame[i + 1], previousFrame[i + 2]);
    const currLuminocity = this.luminocity(currentFrame[i], 
    currentFrame[i + 1], currentFrame[i + 2]);
    if (Math.abs(prevLuminocity - currLuminocity) > threshold) {
        pixels.push({
            x: (i / 4) % width,
            y: Math.floor(i / 4 / width)
                    });
    }
 }
 return pixels;
}</pre>
                    </div>
                    <div style="flex: 1; display: flex; flex-direction: column; gap: 2rem;">
                        <div style="text-align: center;">
                            <h3 style="margin-bottom: 1rem;">Original Video</h3>
                            <video width="100%" controls>
                                <source src="street.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                        <div style="text-align: center;">
                            <h3 style="margin-bottom: 1rem;">Motion Detection</h3>
                            <video width="100%" controls>
                                <source src="street-motion-detection.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Object Detection Introduction -->
        <div class="slide">
            <div class="slide-content">
                <h2>Object Detection with OpenCV and YOLO</h2>
                <ul>
                    <li>OpenCV (Open Computer Vision):
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Industry-standard framework for image processing</li>
                            <li>Provides essential tools for preprocessing and analysis</li>
                            <li>Optimized for real-time applications</li>
                            <li>Extensive library of algorithms for image transformation and feature detection</li>
                        </ul>
                    </li>
                    <li>YOLOv3 (You Only Look Once):
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>State-of-the-art real-time object detection system</li>
                            <li>Single neural network predicts bounding boxes and class probabilities</li>
                            <li>Can detect 80+ object classes in real-time (COCO dataset)</li>
                            <li>Processes images in a single forward pass, making it extremely fast</li>
                        </ul>
                    </li>
                    <li>Performance and Applications:
                        <ul style="margin-left: 2rem; margin-top: 0.25rem;">
                            <li>Achieves 45 FPS on modern GPUs while maintaining high accuracy</li>
                            <li>Effective at detecting small objects and objects in groups</li>
                            <li>Used in autonomous vehicles, surveillance systems, and retail analytics</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>


        <!-- Object Detection Implementation -->
        <div class="slide">
            <div class="slide-content">
                <h2>Object Detection implementation - object_detector.cpp</h2>
                <div style="display: flex; gap: 2rem;">
                    <div style="flex: 1;">
                        <ul>
                            <li>Implementation uses OpenCV's Deep Neural Network (DNN) module with YOLO (You Only Look
                                Once) model</li>
                            <li>Processes video frames as JPEG images through stdin, enabling stream processing</li>

                            <li>Real-time object detection with configurable confidence threshold</li>

                            <li>JSON output format for easy integration</li>

                            <li>Runs neural network inference</li>
                            <li>Processes detections and apply confidence filtering</li>
                            <li>Outputs bounding boxes with class labels and confidence scores</li>

                        </ul>
                    </div>
                    <div style="flex: 1;">
                        <pre class="code-block">
// Sample detection output format
[
    {
        "token": "person",
        "height": 380,
        "x": 250,
        "y": 120,
        "width": 200,
        "percent": 98
    },
    {
        "token": "car",
        "height": 150,
        "x": 400,
        "y": 300,
        "width": 250,
        "percent": 95
    }
]</pre>
                    </div>
                </div>
            </div>
        </div>



    </div>

    <div class="nav">
        <button onclick="previousSlide()" id="prevBtn">Previous</button>
        <span class="slide-counter" id="slideCounter">1 / 10</span>
        <button onclick="nextSlide()" id="nextBtn">Next</button>
    </div>

    <script>
        // Initialize Mermaid
        mermaid.initialize({ startOnLoad: true });

        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');
        const counter = document.getElementById('slideCounter');
        const progressBar = document.getElementById('progressBar');

        function updateSlides() {
            slides.forEach((slide, index) => {
                slide.classList.remove('active', 'previous');
                if (index === currentSlide) {
                    slide.classList.add('active');
                } else if (index < currentSlide) {
                    slide.classList.add('previous');
                }
            });

            // Update counter and buttons
            counter.textContent = `${currentSlide + 1} / ${totalSlides}`;
            prevBtn.disabled = currentSlide === 0;
            nextBtn.disabled = currentSlide === totalSlides - 1;

            // Update progress bar
            const progress = ((currentSlide + 1) / totalSlides) * 100;
            progressBar.style.width = `${progress}%`;
        }

        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                currentSlide++;
                updateSlides();
            }
        }

        function previousSlide() {
            if (currentSlide > 0) {
                currentSlide--;
                updateSlides();
            }
        }

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                nextSlide();
            } else if (e.key === 'ArrowLeft') {
                previousSlide();
            }
        });

        // Initialize progress bar
        updateSlides();
    </script>
</body>

</html>